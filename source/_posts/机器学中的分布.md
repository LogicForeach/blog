---
title: 机器学习中的“分布”
date: {{ date }}
categories:
 - 深度学习
tags:
 - 深度学习
 - 概率分布
toc: true
mathjax: true
---

## 前言

从在输入空间中的一个具体的实例上抽出一些属性，将这些属性组成一组向量，这组向量被称为特征向量。当一个特征向量输入到“学习器”中，“学习器”可以依据输入的特征向量返还一个期望的结果。用通俗的话讲，可以将“学习器”视为一个函数，建立一个输入空间到输出空间的映射，映射规则是由不断训练建立起来的。

一个特征向量所期望的结果，被称为标签。在学习器的训练阶段，我们将特征向量与该特征向量对应的标签一同输入到学习器内，学习器根据自身输出与期望输出的“距离”进行自我优化，不断缩小输出与期望的差距。

考虑到学习器的泛化问题，我们并不要求学习器的输出与期望完全一致，通常我们设立一个相对较“软”的指标。比如对于分类任务而言，我们并不要求学习器输出具体的类别，而是要求学习器输出该特征向量所描述的实例属于某个类的概率是多少；对于回归任务而言，我们期望输出值与期望值尽可能的接近而非完全相同。

习惯上讲特征向量组成的空间称为“特征空间”，也叫”样本空间“，所以特征向量又称为“样本”。在西瓜书中写道：

> 通常假设样本空间中全体样本服从一个未知的“分布”（distribution），我们获得的每个样本都是独立地从这个分布上采样获得的，即“独立同分布”（independent and identically distributed，简称 i.i.d.） ——《机器学习》 周志华

那么问题来了，什么叫分布？什么叫独立同分布？为什么要这样假设？

本文即是我个人对机器学习中“分布”的理解。

## 什么叫分布？

我试图从概率分布对分布进行理解，要理解概率分布，先要搞清楚一个名词——随机变量

### 随机变量

> 随机变量（random variable）表示随机试验各种结果的实值单值函数。随机事件不论与数量是否直接有关，都可以数量化，即都能用数量化的方式表达。  
>
> 随机事件数量化的好处是可以用数学分析的方法来研究随机现象。例如某一时间内公共汽车站等车乘客人数，电话交换台在一定时间内收到的呼叫次数，灯泡的寿命等等，都是随机变量的实例。

简单来说，随机变量就是把事件抽象为一个数值，这个数值可以是事件的结果、事件的编号、事件的属性等。同一个事件，从不同角度进行抽象，将得到不同的随机变量。比如“一枚灯泡的寿命”，如果从单枚灯泡的角度进行抽象，那么寿命可以为 1年，2年，10年，这里 1 2 10 就是一组随机变量，如果从不同类型的灯泡角度抽象，1号灯泡寿命到达10年，2号灯泡寿命到达10，4号灯泡寿命到达10年，这里的 1 2 4 又是一组随机变量，这两组随机变量因为意义不同，所以是不能放在一起讨论的，所以只有相同意义的随机变量才能放在一起讨论，下面概率分布中所提及的随机变量，都是指意义相同的随机变量。

### 概率分布

概率分布，是指用于表述随机变量取值的概率规律。将随机变量作为横轴，概率作为纵轴，把随机变量与对应变量画上去，构成一个图形，这个图像就是概率分布的直观表示。通常也用概率分布函数表示 $F(x)$ 来描述一个概率分布，概率分布函数被定义为：
$$
F(x)=P\{X<x\}
$$
总之概率分布也可以理解为一个函数，它刻画了随机变量与概率的映射关系，给定一个概率分布，就可以求任何随机变量对应的概率了。当一个随机变量与它的概率满足某一个概率分布的映射关系时，则称这个随机变量服从该概率分布。

### 机器学习中的 “分布” 是概率分布吗？

前文西瓜书中所提及的分布，即为概率分布，指每个样本从样本空间中被抽到的概率遵循统一的概率分布。为了说明，假设一个样本空间的抽样服从以下概率分布：

| $\pmb X$ |  猫  |  狗  |
| :------: | :--: | :--: |
|  **P**   | 0.5  | 0.5  |

这里的 $\pmb X$ 是表示从样本空间的任抽取一例所得结果的随机变量，可取 { 猫，狗 } ；**P** 是随机变量对应的概率。

假设样本空间的随机变量  $\pmb X$ 服从上述概率分布，从样本空间中独立地、按照上述分布地进行抽样，最终得到得到 n 个样本 $(\pmb X_1,\pmb X_2,\dots,\pmb X_n)$ , 其中 $\pmb X_i$ 是相互独立且均与 $\pmb X$ 服从同一概率分布的随机变量，所以 $\pmb X_i$ 的取值范围和每个取值对应的概率都与 $\pmb X$ 相同。

也就是说在独立同分布的假设下，每个样本取到猫还是取到狗的概率都各占 0.5 ，随着样本数目的增多，在所有样本中任取一个样本是猫是狗的概率分布也将趋近于总体的概率分布，如此而来，虽然样本数目远不及总体，但可以尽可能全面地表现总体的特征，这便是独立同分布的意义。

### 如何理解 “分布” 的概念

将随机变量的取值与其对于的概率，画在随机变量为横轴、概率为纵轴的直角坐标系上，所有点构成的图像，即为概率分布的一个直观形象。也可以说，概率分布就是指这个图像。在直角坐标系上的一个图像，又对应着横轴与纵轴的一种映射关系，所以也可以说，概率分布是随机变量与概率的映射关系。那么什么是 “分布” 呢，**“分布” 即是一个描述直角坐标系上图像的词，一个描述映射关系的词**。如：频数分布，其实是在描述随机变量与频数的映射，XX分布，就是描述某一变量与XX的映射。总结一下就是：**分布即图像、分布即映射** 。

首先需要理解，什么是叫独立，什么叫同分布。

独立好理解，就是指一个特征向量的结果不影响另外一个特征向量，

同分布按照我的理解就是指所有特征向量与其期望的输出都具有相同的映射关系，即这个映射关系在这个样本集唯一。

**为什么要求独立呢？**

想想也是，我们最终学得的一个“学习器”，这个“学习器”被定义为一个特征向量与模型输出的映射关系，如果一个特征向量的结果，受到另一个特征向量的影响，此时两个特征向量不独立，如果我们想得到一个正确的输出需要把两个特征向量都输入进去，否则学习器会困惑，具体一点的例子就是，把一张只有猫尾巴的图片作为训练素材输入给一个“猫狗分类的学习器”进行训练，最后学习器肯定得不到一个好结果。

**为什么要求同分布呢？**

也是这个道理，大多数“学习器”只能在大量训练中，学会一种映射关系，比如猫狗分类任务，”学习器“的输出非猫即狗，这时候混几张兔子的素材进去，“学习器”准确度肯定要下降，此时对于猫狗素材而言，它的特征向量与期望输出存在一种映射关系，对于兔子素材而言，它的特征向量与期望输出存在的映射关系，一定与猫狗素材的不同，此时训练集的样本就没有满足同分布。

所以一个优质的训练集，样本独立同分布是一个必不可少的前提，如果样本不独立，那么学习器学不到完整的特征，如果样本不同分布，则学习会在不同映射规则之中跳来跳去。独立同分布少了哪个都会让学习器变的不稳定。

## BN 论文中所提及的 “分布”

“分布” 这个词另一个提出场景，就是在 BN 解决 ICS 的时候，

> 在文章[Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)中，其对BN与ICS是这样解释的：由于前一层的参数更新，所以这一层的输入（前一层的输出）的分布会发生变化，这种现象被称之为ICS。同样，这篇文章的观点认为BN  work的真正原因，在与其将数据的分布都归一化到均值为0，方差为1的分布上去。因此，每一层的输入（上一层输出经过BN后）分布的稳定性都提高了，故而整体减小了网络的ICS。
>
> 原文：https://zhuanlan.zhihu.com/p/52749286

这里的分布听上去也不像概率分布，更像最大最小值区间，所以可以理解为一个特征向量到特征向量所在区间的映射，神经网络的映射规则是由特征向量与一组参数的组合运算实现，总会有一些神经元对特征向量的变化很敏感，如果期望输出不变，特征向量所在区间发送了较大的改变，说明特征向量数值波动较大，那么这些神经元可能无法很好的拟合这个映射规则，所以要通过 BN 把所有特征向量都固定在一个相对集中的区间。

顺嘴一起提，已有论文证明，BN 其实与 ICS 无关，参考链接：

https://zhuanlan.zhihu.com/p/52749286