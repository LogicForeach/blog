---
title: 目标检测中的损失函数笔记整理(待续)
date: {{ date }}
categories:
 - 深度学习
tags:
 - 深度学习
 - 目标检测
 - 损失函数
toc: true
mathjax: true
---

##  Smooth L1 Loss

### 函数定义与性质：

#### 定义

$$
smoothL1(x) = \left\{ \begin{aligned} 0.5x^{2}    \qquad if \left| x \right| < 1\\ \left| x \right| - 0.5  \qquad otherswise,  \\ \end{aligned} \right.
$$



#### 性质

+ 连续函数，在 x=±1 处连续
+ 偶函数，关于 x=0 对称
+ x=0 处取唯一极小值 0

### 导数定义与性质：

#### 定义

$$
\frac{d[smoothL1(x)]}{x}= \left\{ \begin{aligned} x \qquad if \left| x \right| <1\\ \pm1  \qquad otherswise,  \\ \end{aligned} \right.
$$

#### 性质

+ 导函数连续
+ 在 x 较小时，对 x 的梯度也会变小，而在 x 很大时，对 x 的梯度为 1 ，总之 x 梯度绝对值最大为 1 

### 来源：

在 Faster RCNN 用于对 （x，y，w，h）进行回归：
$$
L_{reg}(t_i,t_i^*)=smoothL1(t_i-t_i^*)
\\t_i=(x_i,y_i,w_i,h_i)
\\t_i^*=(x_i^*,y_i^*,w_i^*,h_i^*)
$$

### 优缺点分析：

#### 优点

+ 相较于平方误差， Smooth L1 Loss 的在预测值与标签值相差较大时，梯度被稳定为 1，这样不会让那些少量离群点贡献较大的梯度，令整个梯度下降方向，向那些错误的点倾斜。
+ 相较于绝对值误差，Smooth L1 Loss 在 0 点处可导，且预测值与真实值相差的越小，梯度也越小，这将有利于模型收敛
+ .....（待补充）

#### 缺点

+ 在目标识别时，（x，y，w，h）的回归损失采用加和方式组合在一起，使得（x，y，w，h）的损失彼此独立，导致 loss 虽然看起来不错，但是（x，y，w，h）四项的回归损失有高有低，这种情况下的结果通常最终效果的评价都不是很好（最终效果的评价采用 IOU 作为标准）。
+ 目标识别时，使用 Smooth L1 Loss 的话，大边框的 loss 和小边框的 loss 并不能同等比较，相同 loss 值的情况下，大边框（100\*100）的宽和高差2个像素点，肯定比小边框（10\*10）的宽和高差2个像素点效果要好，但是 Smooth L1 Loss 中没有考虑边框大小这个问题，在 YOLO 中，作者使用平方根函数缓解这个问题，可这个问题依旧存在。换句话说就是  Smooth L1 Loss 不具有尺度不变形

<!-- more -->

## IOU Loss

### 函数定义与性质

#### 定义

$$
IOU(A,B)=\frac{A\cap B}{A\cup B}
\\L_{reg}(PB,GT)=-ln[IOU(PB,GT)]
$$

或
$$
L_{reg}(PB,GT)=1-IOU(PB,GT)
$$

#### 性质

+ 非负，有下届，交并比为1，损失函数为 0
+ 使用交并比衡量预测数据与真实数据的“距离”

#### 计算流程

+ 记 $A,B$  代表两个不同的矩形
+ 设  $A\cap B=I,A\cup B=U$ , 则 $IOU=\frac{I}{U}$
+ 因 $U=A\cup B=A+B-I$ , 所以通常使用 $IOU=\frac{I}{A+B-I}$ 计算
+ 若 $A=(A_{top},A_{bottom},A_{left},A_{right}),B=(B_{top},B_{bottom},B_{left},B_{right})$
+ 则 $I=(min\{A_{bottom},B_{bottom}\}-(max\{A_{top},B_{top}\})*(min\{A_{right},B_{right}\}-max\{A_{left},B_{left}\})$

### 导数的定义与性质

$$
\frac{\partial L}{\partial A_{top}}=\frac 1 {IOU} \cdot \frac{\partial IOU}{\partial A_{top}}
$$



#### 性质

+ 交并比为零时不可导，即预测框与真实框不相交则不可导
+ 随交并比的变大，梯度也在减小
+ 梯度计算复杂，预测值相互影响，收敛速度慢

### 来源

在《UnitBox: An Advanced Object Detection Network》中被提出并使用

### 优缺点分析

#### 优点

+ 具有尺度不变性，因为本身就是比例，所以没有量纲，大框小框都是一视同仁
+ IOU 强调了大小、位置、宽高之间的联系，loss 值和效果评价指标（如 mAP）关联性更强

#### 缺点

+ 当预测框和真实框没有交集的时候，IOU=0，是一个常数，没有梯度，这个在两阶段目标检测网络中问题不大，因为没有交集的预测框通常都被过滤掉了，但是对于 YOLO 这样的一阶段网络来说，这些远离真实框的预测框失去了向真实框靠近的能力，虽然最后也会被过滤掉，但是会导致更少的预测框向真实框回归，间接导致准确度下降。
+ IOU 实际上是一个关于相交面积的函数，仅与相交面积有关，在相交面积相同的情况下，还有很多效果的不同的相交方式，这些相交方式之间无法平均。
+ 收敛速度慢

## GIOU Loss

### 函数定义与性质

$$
IOU(A,B)=\frac{A\cap B}{A\cup B}
\\GIOU(A,B)=IOU(A,B)-\frac{C-(A\cup B)}{C}
\\L_{reg}(PB,GT)=1-GIOU(PB,GT)
$$



其中 $C$ 是 $A,B$ 的最小外接矩形的面积

#### 计算流程

+ 记 $A,B$  代表两个不同的矩形的面积
+ 其中 $A=(A_{top},A_{bottom},A_{left},A_{right}),B=(B_{top},B_{bottom},B_{left},B_{right})$
+ 则 $I=(min\{A_{bottom},B_{bottom}\}-(max\{A_{top},B_{top}\})*(min\{A_{right},B_{right}\}-max\{A_{left},B_{left}\})$
+ $U=A+B-I$
+ $C=(max\{A_{bottom},B_{bottom}\}-(min\{A_{top},B_{top}\})*(max\{A_{right},B_{right}\}-min\{A_{left},B_{left}\})$
+ $GIOU=\frac{I}{U}-\frac{C-(U)}{C}$

#### 性质

+ 具有尺度不变性
+ -1<=GIOU<=1，当A=B时，GIOU=IOU=1；当A与B不相交而且离得很远时，GIOU(A,B) 趋向于-1。即当 A 和 B 不相交时，GIOU 不像 IOU 那样是一个常数。

### 导数性质

#### 性质

+ 不相交的矩形 A 、B 的 GIOU 仍具有梯度

### 来源

在《Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression》中被提出

### 优缺点分析

#### 优点

+ 即便不相交，也有梯度，适用于“一步走”的目标识别算法

#### 缺点

+ 当两个框为包含关系时，GIOU 等同于 IOU 对回归情况还是不能很好的评价

## DIOU Loss

### 函数定义与性质

$$
DIOU(B,B^{gt}) = IOU(B,B^{gt}) - \frac{\rho^{2}(b,b^{gt})}{c^{2}}
\\L_{reg}(B,B^{gt})=1-DIOU(B,B^{gt})
$$

其中 $\rho(\cdot)$ 表示欧几里得距离，$b,b^{gt}$ 分别表示矩形框 $B,B^{gt}$ 的中心点坐标，$c$ 表示 $B,B^{gt}$ 的最小外接矩形的对角线长度。

#### 性质

+ 具有尺度不变性
+ -1<=DIOU<=1，A 和 B 完全重合时，DIOU=1；当 A 和 B 为包含关系时，A 和 B 的中心点距离越近 DIOU 越大；当 A 和 B 完全不相交时，A 和 B 中心点距离越远， DIOU 越小，最终趋近 -1

### 导数性质

#### 性质

+ 不相交的矩形 A 、B 的 GIOU 仍具有梯度
+ 相比于 GIOU，DIOU收敛的更快

## CIOU Loss

### 函数定义与性质

$$
CIOU(B,B^{gt})=DIOU(B,B^{gt})-\alpha\upsilon
\\L_{reg}(B,B^{gt})=1-CIOU(B,B^{gt})
$$

其中
$$
\alpha=\frac{\upsilon}{(1-IOU)+\upsilon}\\ 
 \upsilon = \frac{4}{\pi ^2}\left(arctan\frac{w^{gt}}{h^{gt}}-arctan\frac{w}{h} \right)^2
$$

### 优缺点分析

#### 优点

+ 在 DIOU 基础上，考虑了长宽比的回归

